{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934eb52d-0cb1-4ad2-b224-5a56753ce9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd \n",
    "import math\n",
    "import numpy as np\n",
    "import torch, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import evaluate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef8e6c97-8727-474d-a756-a3b50ea44cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths for the original version of the model with sentences format of clauses with 4 taxonomic level \n",
    "OUTPUT_DIR_sample_type = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/Sample_type\"\n",
    "OUTPUT_DIR_age = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/age\"\n",
    "DATA_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd00f75-97fc-4fed-bf13-00a915ccedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the version of the model with sentences with only species level\n",
    "# OUTPUT_DIR_sample_type = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/Sample_type\"\n",
    "# OUTPUT_DIR_age = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/age\"\n",
    "# DATA_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels_species_level\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31eab991-0920-4e0d-8355-84f5c25e0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "TSV_FILE = f\"{DATA_DIR}/data_for_fine_tuning.tsv\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TARGET_LABEL_age = 'Age'  # Change this to 'Age' if you want to predict age / Sample_type\n",
    "TARGET_LABEL_sample_type = 'Sample_type' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ba2531-8e66-45d3-91af-352b8314ee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### age label ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38d3adc-c103-4943-bff1-9f2574feb626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading TSV file...\n",
      "Loaded 5950 samples\n",
      "Columns: ['Filename', 'Sentence', 'Location', 'Sample_type', 'Age', 'Lifestyle']\n",
      "\n",
      "First 3 samples:\n",
      "           Filename                                           Sentence    Age\n",
      "0  Zeevi_ERR1110297  GGB28271_SGB40830 GGB28262_SGB40814 Phocaeicol...  adult\n",
      "1  Zeevi_ERR1110298  GGB1364_SGB1834 Alistipes_putredinis Bacteroid...  adult\n",
      "2  Zeevi_ERR1110299  Bacteroides_uniformis GGB1627_SGB2230 Phocaeic...  adult\n",
      "\n",
      "Samples after removing NaN: 5950\n",
      "\n",
      "Age distribution:\n",
      "Age\n",
      "adult    2935\n",
      "1-3Y     1217\n",
      "6-12M     569\n",
      "1-4M      411\n",
      "4-6M      373\n",
      "0-1M      346\n",
      "child      99\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping:\n",
      "  0-1M: 0\n",
      "  1-3Y: 1\n",
      "  1-4M: 2\n",
      "  4-6M: 3\n",
      "  6-12M: 4\n",
      "  adult: 5\n",
      "  child: 6\n",
      "\n",
      "Total samples: 5950\n",
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "# Read the TSV file\n",
    "print(\"Reading TSV file...\")\n",
    "df = pd.read_csv(TSV_FILE, sep='\\t')\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "print(df[['Filename', 'Sentence', TARGET_LABEL_age]].head(3))\n",
    "\n",
    "# Remove samples with missing target labels or sentences\n",
    "df_clean = df.dropna(subset=['Sentence', TARGET_LABEL_age])\n",
    "print(f\"\\nSamples after removing NaN: {len(df_clean)}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(f\"\\n{TARGET_LABEL_age} distribution:\")\n",
    "print(df_clean[TARGET_LABEL_age].value_counts())\n",
    "\n",
    "# Prepare labels\n",
    "unique_labels = sorted(df_clean[TARGET_LABEL_age].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"\\nLabel mapping:\")\n",
    "for label, id in label2id.items():\n",
    "    print(f\"  {label}: {id}\")\n",
    "\n",
    "# Convert labels to numeric\n",
    "df_clean['labels'] = df_clean[TARGET_LABEL_age].map(label2id)\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df_clean['Sentence'].tolist()\n",
    "labels = df_clean['labels'].tolist()\n",
    "\n",
    "print(f\"\\nTotal samples: {len(texts)}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"text\": texts,\n",
    "    \"labels\": labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d95adfa1-f23d-4ede-96a6-6081fd99bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 new tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize_function at 0x7f2da35b1c60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized embeddings to: torch.Size([30526, 768])\n",
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d2226be304419795448de1e74ca166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset: Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 5950\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Add custom tokens\n",
    "new_tokens = ['acea', 'ales', 'um', 'bacter', 'coccus', 'bacill']\n",
    "num_added = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"Added {num_added} new tokens.\")\n",
    "\n",
    "# Resize model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized embeddings to: {model.get_input_embeddings().weight.shape}\")\n",
    "\n",
    "# OPTIONAL: freeze all other parameters (only train embeddings)\n",
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze classifier\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze last 2 transformer layers (adjust number as needed)\n",
    "for param in model.bert.encoder.layer[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze new token embeddings\n",
    "model.get_input_embeddings().weight.requires_grad = True\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "print(f\"Tokenized dataset: {train_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb0e527a-e169-4d56-9d91-b2d70d0f13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation metrics\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23214fb-ca20-444d-ba5b-9a436f57d2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps per epoch: 185\n",
      "Total training steps: 555\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1358321/175085756.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 02:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.939900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.869000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.751000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.703100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.695200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=558, training_loss=0.8465914375893104, metrics={'train_runtime': 160.9818, 'train_samples_per_second': 110.882, 'train_steps_per_second': 3.466, 'total_flos': 4696743179520000.0, 'train_loss': 0.8465914375893104, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_age, # if age label - OUTPUT_DIR_age / OUTPUT_DIR_sample_type\n",
    "    eval_strategy=\"no\",  # No evaluation during training (training on full dataset)\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,  # Common learning rate for classification\n",
    "    fp16=True,\n",
    "    seed=SEED,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=f\"{OUTPUT_DIR_age}/logs\", # if age label - OUTPUT_DIR_age / OUTPUT_DIR_sample_type\n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "print(f\"\\nSteps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total training steps: {steps_per_epoch * training_args.num_train_epochs}\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b3239a4-f203-4501-aec5-5cd4bfddc9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/age/tokenizer/tokenizer_config.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/age/tokenizer/special_tokens_map.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/age/tokenizer/vocab.txt',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/age/tokenizer/added_tokens.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/age/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Age:\n",
    "model.save_pretrained(f\"{OUTPUT_DIR_age}/model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR_age}/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11c0d16a-6811-4510-bdc2-c86b1827f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save label mappings - age\n",
    "with open(f\"{OUTPUT_DIR_age}/label_mappings.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"label2id\": label2id,\n",
    "        \"id2label\": id2label,\n",
    "        \"target_label\": TARGET_LABEL_age\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5ea474a-e33c-4f56-950a-0c21dc26195f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentences(sentences, model, tokenizer, id2label):\n",
    "    \"\"\"\n",
    "    Predict labels for given sentences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        encoded = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Get probabilities\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "            # Get predicted class\n",
    "            predicted_class_id = torch.argmax(logits, dim=-1).item()\n",
    "            predicted_label = id2label[predicted_class_id]\n",
    "\n",
    "            predictions.append(predicted_label)\n",
    "            probabilities.append(probs.cpu().numpy()[0])\n",
    "\n",
    "    return predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc28a0e0-3d08-4e16-b05c-17fcfdcbb791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for dataset examples:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "Text: Veillonella_ratti Collinsella_sp_AK_207A Bifidobacterium_longum Bifidobacterium_bifidum Bifidobacter...\n",
      "True Age: 6-12M\n",
      "Predicted Age: 4-6M\n",
      "Confidence: 0.3522\n",
      "Correct: ✗\n",
      "\n",
      "Example 2:\n",
      "Text: Bifidobacterium_bifidum Limosilactobacillus_mucosae GGB4266_SGB5809 Megamonas_funiformis Megasphaera...\n",
      "True Age: 6-12M\n",
      "Predicted Age: 4-6M\n",
      "Confidence: 0.3773\n",
      "Correct: ✗\n",
      "\n",
      "Example 3:\n",
      "Text: Klebsiella_grimontii Enterobacter_kobei Enterococcus_faecalis Veillonella_parvula Robinsoniella_peor...\n",
      "True Age: 4-6M\n",
      "Predicted Age: 0-1M\n",
      "Confidence: 0.5498\n",
      "Correct: ✗\n"
     ]
    }
   ],
   "source": [
    "# Get a few random examples from your dataset\n",
    "sample_indices = np.random.choice(len(texts), size=3, replace=False)\n",
    "sample_texts = [texts[i] for i in sample_indices]\n",
    "sample_true_labels = [id2label[labels[i]] for i in sample_indices]\n",
    "\n",
    "# Make predictions\n",
    "sample_predictions, sample_probabilities = predict_sentences(sample_texts, model, tokenizer, id2label)\n",
    "\n",
    "print(f\"\\nPredictions for dataset examples:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (text, true_label, pred, probs) in enumerate(zip(sample_texts, sample_true_labels, sample_predictions, sample_probabilities)):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: {text[:100]}...\")  # Show first 100 characters\n",
    "    print(f\"True {TARGET_LABEL_age}: {true_label}\")\n",
    "    print(f\"Predicted {TARGET_LABEL_age}: {pred}\")\n",
    "    print(f\"Confidence: {max(probs):.4f}\")\n",
    "\n",
    "    # Check if prediction is correct\n",
    "    is_correct = \"✓\" if pred == true_label else \"✗\"\n",
    "    print(f\"Correct: {is_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259f066-19f6-4c9b-822e-94f2fda16362",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### sample type label #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3178af3f-8372-4a40-a4e2-074e3f11cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading TSV file...\n",
      "Loaded 5950 samples\n",
      "Columns: ['Filename', 'Sentence', 'Location', 'Sample_type', 'Age', 'Lifestyle']\n",
      "\n",
      "First 3 samples:\n",
      "           Filename                                           Sentence  \\\n",
      "0  Zeevi_ERR1110297  GGB28271_SGB40830 GGB28262_SGB40814 Phocaeicol...   \n",
      "1  Zeevi_ERR1110298  GGB1364_SGB1834 Alistipes_putredinis Bacteroid...   \n",
      "2  Zeevi_ERR1110299  Bacteroides_uniformis GGB1627_SGB2230 Phocaeic...   \n",
      "\n",
      "  Sample_type  \n",
      "0   adult_gut  \n",
      "1   adult_gut  \n",
      "2   adult_gut  \n",
      "\n",
      "Samples after removing NaN: 5950\n",
      "\n",
      "Sample_type distribution:\n",
      "Sample_type\n",
      "infant_gut    2916\n",
      "adult_gut     1679\n",
      "vaginal       1256\n",
      "child_gut       99\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Label mapping:\n",
      "  adult_gut: 0\n",
      "  child_gut: 1\n",
      "  infant_gut: 2\n",
      "  vaginal: 3\n",
      "\n",
      "Total samples: 5950\n",
      "Number of classes: 4\n"
     ]
    }
   ],
   "source": [
    "# Read the TSV file\n",
    "print(\"Reading TSV file...\")\n",
    "df = pd.read_csv(TSV_FILE, sep='\\t')\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "print(df[['Filename', 'Sentence', TARGET_LABEL_sample_type]].head(3))\n",
    "\n",
    "# Remove samples with missing target labels or sentences\n",
    "df_clean = df.dropna(subset=['Sentence', TARGET_LABEL_sample_type])\n",
    "print(f\"\\nSamples after removing NaN: {len(df_clean)}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(f\"\\n{TARGET_LABEL_sample_type} distribution:\")\n",
    "print(df_clean[TARGET_LABEL_sample_type].value_counts())\n",
    "\n",
    "# Prepare labels\n",
    "unique_labels = sorted(df_clean[TARGET_LABEL_sample_type].unique())\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"\\nLabel mapping:\")\n",
    "for label, id in label2id.items():\n",
    "    print(f\"  {label}: {id}\")\n",
    "\n",
    "# Convert labels to numeric\n",
    "df_clean['labels'] = df_clean[TARGET_LABEL_sample_type].map(label2id)\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df_clean['Sentence'].tolist()\n",
    "labels = df_clean['labels'].tolist()\n",
    "\n",
    "print(f\"\\nTotal samples: {len(texts)}\")\n",
    "print(f\"Number of classes: {len(unique_labels)}\")\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"text\": texts,\n",
    "    \"labels\": labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b56d3b7-f46e-45c9-8261-28cad490a721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 new tokens.\n",
      "Resized embeddings to: torch.Size([30526, 768])\n",
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd770ff14627433886a27e37fe26c272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset: Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 5950\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Add custom tokens\n",
    "new_tokens = ['acea', 'ales', 'um', 'bacter', 'coccus', 'bacill']\n",
    "num_added = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"Added {num_added} new tokens.\")\n",
    "\n",
    "# Resize model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized embeddings to: {model.get_input_embeddings().weight.shape}\")\n",
    "\n",
    "# OPTIONAL: freeze all other parameters (only train embeddings)\n",
    "# Freeze all parameters first\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze classifier\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze last 2 transformer layers (adjust number as needed)\n",
    "for param in model.bert.encoder.layer[-2:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Unfreeze new token embeddings\n",
    "model.get_input_embeddings().weight.requires_grad = True\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Tokenize dataset\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset.set_format(\"torch\")\n",
    "\n",
    "print(f\"Tokenized dataset: {train_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b38191c0-7c2d-4f3c-9d10-3c04bf99b861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Steps per epoch: 185\n",
      "Total training steps: 555\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1358321/371135025.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 02:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.480100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.297800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.215900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.188500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=558, training_loss=0.3227600805648339, metrics={'train_runtime': 156.1072, 'train_samples_per_second': 114.344, 'train_steps_per_second': 3.574, 'total_flos': 4696616674713600.0, 'train_loss': 0.3227600805648339, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR_sample_type, \n",
    "    eval_strategy=\"no\",  # No evaluation during training (training on full dataset)\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,  # Common learning rate for classification\n",
    "    fp16=True,\n",
    "    seed=SEED,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=f\"{OUTPUT_DIR_sample_type}/logs\", \n",
    "    logging_steps=50,\n",
    "    logging_first_step=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=2,\n",
    ")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "print(f\"\\nSteps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total training steps: {steps_per_epoch * training_args.num_train_epochs}\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aedb976d-710a-4c09-891a-8f71cb32ba81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/Sample_type/tokenizer/tokenizer_config.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/Sample_type/tokenizer/special_tokens_map.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/Sample_type/tokenizer/vocab.txt',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/Sample_type/tokenizer/added_tokens.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/Sample_type/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample_type:\n",
    "model.save_pretrained(f\"{OUTPUT_DIR_sample_type}/model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR_sample_type}/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21557d84-683f-43f4-bf3e-77aebba98602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save label mappings - sample type\n",
    "with open(f\"{OUTPUT_DIR_sample_type}/label_mappings.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"label2id\": label2id,\n",
    "        \"id2label\": id2label,\n",
    "        \"target_label\": TARGET_LABEL_sample_type\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9dab24a-5b32-4417-b27a-d014e1e65d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for dataset examples:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 1:\n",
      "Text: Veillonella_ratti Collinsella_sp_AK_207A Bifidobacterium_longum Bifidobacterium_bifidum Bifidobacter...\n",
      "True Sample_type: infant_gut\n",
      "Predicted Sample_type: infant_gut\n",
      "Confidence: 0.9881\n",
      "Correct: ✓\n",
      "\n",
      "Example 2:\n",
      "Text: Bifidobacterium_bifidum Limosilactobacillus_mucosae GGB4266_SGB5809 Megamonas_funiformis Megasphaera...\n",
      "True Sample_type: infant_gut\n",
      "Predicted Sample_type: infant_gut\n",
      "Confidence: 0.9884\n",
      "Correct: ✓\n",
      "\n",
      "Example 3:\n",
      "Text: Klebsiella_grimontii Enterobacter_kobei Enterococcus_faecalis Veillonella_parvula Robinsoniella_peor...\n",
      "True Sample_type: infant_gut\n",
      "Predicted Sample_type: infant_gut\n",
      "Confidence: 0.9670\n",
      "Correct: ✓\n"
     ]
    }
   ],
   "source": [
    "# Get a few random examples from your dataset\n",
    "sample_indices = np.random.choice(len(texts), size=3, replace=False)\n",
    "sample_texts = [texts[i] for i in sample_indices]\n",
    "sample_true_labels = [id2label[labels[i]] for i in sample_indices]\n",
    "\n",
    "# Make predictions\n",
    "sample_predictions, sample_probabilities = predict_sentences(sample_texts, model, tokenizer, id2label)\n",
    "\n",
    "print(f\"\\nPredictions for dataset examples:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (text, true_label, pred, probs) in enumerate(zip(sample_texts, sample_true_labels, sample_predictions, sample_probabilities)):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: {text[:100]}...\")  # Show first 100 characters\n",
    "    print(f\"True {TARGET_LABEL_sample_type}: {true_label}\")\n",
    "    print(f\"Predicted {TARGET_LABEL_sample_type}: {pred}\")\n",
    "    print(f\"Confidence: {max(probs):.4f}\")\n",
    "\n",
    "    # Check if prediction is correct\n",
    "    is_correct = \"✓\" if pred == true_label else \"✗\"\n",
    "    print(f\"Correct: {is_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04548f-e091-423c-aac1-b2d51208de8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp_project_jupyter_env",
   "language": "python",
   "name": "anlp_project_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
