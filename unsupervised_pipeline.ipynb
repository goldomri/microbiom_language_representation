{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9472f77a-14aa-4e73-912e-c0923ed1db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd \n",
    "import math\n",
    "import numpy as np\n",
    "import torch, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ffd71c-9efa-400a-bd64-7a961fca1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths for the original version of the model with sentences format of clauses with 4 taxonomic level \n",
    "OUTPUT_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model\"\n",
    "DATA_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51894cd6-6446-4a68-9fe1-e51e5979856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the version of the model with sentences with only species level\n",
    "# OUTPUT_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model_species_level\"\n",
    "# DATA_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels_species_level\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa099e7-06c9-4e24-a253-dd3c1ea12a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for the version of the model with sentences with only species level\n",
    "# OUTPUT_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model_regular_tokenizer\"\n",
    "# DATA_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95eaaea8-0ce4-4b43-be9a-d1863c0b7655",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "TSV_FILE = f\"{DATA_DIR}/data_for_fine_tuning.tsv\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c02447-a5f1-41d7-9131-d841114d6234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6 new tokens.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized embeddings to: torch.Size([30526, 768])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Add custom tokens\n",
    "new_tokens = ['acea', 'ales', 'um', 'bacter', 'coccus', 'bacill']\n",
    "num_added = tokenizer.add_tokens(new_tokens)\n",
    "print(f\"Added {num_added} new tokens.\")\n",
    "\n",
    "# Resize model embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Resized embeddings to: {model.get_input_embeddings().weight.shape}\")\n",
    "\n",
    "# OPTIONAL: freeze all other parameters (only train embeddings)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.get_input_embeddings().weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6424cea2-614d-45e1-b4ae-7a1fccc61b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model     = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# OPTIONAL: freeze all other parameters (only train embeddings)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.get_input_embeddings().weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b4b592-5828-413d-af76-93b0bf599ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading TSV file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f07df5f6440> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5950 samples\n",
      "Columns: ['Filename', 'Sentence', 'Location', 'Sample_type', 'Age', 'Lifestyle']\n",
      "\n",
      "First 3 samples:\n",
      "           Filename                                           Sentence\n",
      "0  Zeevi_ERR1110297  Bacteroidales Bacteroidaceae GGB28271 GGB28271...\n",
      "1  Zeevi_ERR1110298  Bacteroidales Bacteroidaceae GGB1364 GGB1364_S...\n",
      "2  Zeevi_ERR1110299  Bacteroidales Bacteroidaceae Bacteroides Bacte...\n",
      "\n",
      "Total sentences after removing NaN: 5950\n",
      "\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ab6807c5024404ab767dfc19a30336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets prepared successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read the TSV file\n",
    "print(\"Reading TSV file...\")\n",
    "df = pd.read_csv(TSV_FILE, sep='\\t')\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows to verify data\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "print(df[['Filename', 'Sentence']].head(3))\n",
    "\n",
    "# Extract sentences for training\n",
    "sentences = df['Sentence'].dropna().tolist()\n",
    "print(f\"\\nTotal sentences after removing NaN: {len(sentences)}\")\n",
    "train_ds = Dataset.from_dict({\"text\": sentences})\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],\n",
    "                     truncation=True,\n",
    "                     max_length=512,\n",
    "                     return_special_tokens_mask=True)\n",
    "\n",
    "# Map tokenization\n",
    "print(\"\\nTokenizing dataset...\")\n",
    "train_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_ds.set_format(\"torch\")\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm_probability=0.15)\n",
    "\n",
    "print(\"\\nDatasets prepared successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85c8f5f8-a569-436e-ab67-52dcc41ad3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 185\n",
      "Total training steps: 555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='558' max='558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [558/558 03:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.442400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.966200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.894500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.888700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.837300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=558, training_loss=1.0756663284848669, metrics={'train_runtime': 183.72, 'train_samples_per_second': 97.159, 'train_steps_per_second': 3.037, 'total_flos': 4698205908480000.0, 'train_loss': 1.0756663284848669, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    OUTPUT_DIR,\n",
    "    eval_strategy=\"no\",  # No evaluation during training\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,    # effectively 32 samples per step\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    seed=SEED,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    logging_steps=50,  # Log training loss every 50 steps\n",
    "    logging_first_step=True,  # Log the first step\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=2\n",
    ")\n",
    "\n",
    "# Calculate steps per epoch for reference\n",
    "steps_per_epoch = len(train_ds) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Total training steps: {steps_per_epoch * training_args.num_train_epochs}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2db2f0c-61cf-4169-84b7-dfe33e920656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model_regular_tokenizer/tokenizer/tokenizer_config.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model_regular_tokenizer/tokenizer/special_tokens_map.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model_regular_tokenizer/tokenizer/vocab.txt',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model_regular_tokenizer/tokenizer/added_tokens.json',\n",
       " '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/unsupervised_model_regular_tokenizer/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(f\"{OUTPUT_DIR}/model\")\n",
    "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp_project_jupyter_env",
   "language": "python",
   "name": "anlp_project_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
