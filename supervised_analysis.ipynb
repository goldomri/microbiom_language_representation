{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a030fc8-0e19-4288-a83d-34e837c6e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, numpy as np\n",
    "# import seaborn as sns\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd \n",
    "import math\n",
    "import numpy as np\n",
    "import torch, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3394eae-eed3-4994-9069-a0818d9008a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR_GMAP = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/GMAP\"\n",
    "OUTPUT_DIR_MOBILE = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE\"\n",
    "OUTPUT_DIR_sample_type = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/Sample_type\"\n",
    "OUTPUT_DIR_age = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/age\"\n",
    "\n",
    "DATA_DIR = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels\"\n",
    "TSV_FILE_GMAP = f\"{DATA_DIR}/gmap_sentences_and_labels.tsv\"\n",
    "TSV_FILE_MOBILE = f\"{DATA_DIR}/mobile_sentences_and_labels.tsv\"\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28352f28-d6e9-4cc3-9900-4d73da0439f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data_only(tsv_file_path, text_column='Sentence', label_column='Sample_type',\n",
    "                           metadata_columns=['Filename', 'Age', 'Sample_type'],\n",
    "                           existing_label2id=None):\n",
    "    \"\"\"\n",
    "    Prepare only test data using existing label mappings (for when you have a pre-trained model).\n",
    "\n",
    "    Args:\n",
    "        tsv_file_path (str): Path to the test TSV file\n",
    "        text_column (str): Name of the column containing text data\n",
    "        label_column (str): Name of the column containing labels\n",
    "        metadata_columns (list): List of column names to extract as metadata\n",
    "        existing_label2id (dict): Existing label to ID mapping from training\n",
    "\n",
    "    Returns:\n",
    "        tuple: (test_texts, test_labels, metadata_df, unknown_labels, filtered_label2id)\n",
    "    \"\"\"\n",
    "    print(f\"Preparing test data from: {tsv_file_path}\")\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "    print(f\"Loaded {len(df)} test samples\")\n",
    "\n",
    "    # Clean data (remove rows with missing text/label)\n",
    "    df_clean = df.dropna(subset=[text_column, label_column]).reset_index(drop=True)\n",
    "    print(f\"Test samples after cleaning: {len(df_clean)}\")\n",
    "\n",
    "    unknown_labels = []\n",
    "\n",
    "    if existing_label2id is not None:\n",
    "        # Sets of test labels and known labels\n",
    "        unique_test_labels = set(df_clean[label_column].unique())\n",
    "        known_labels = set(existing_label2id.keys())\n",
    "\n",
    "        print(\"Labels from test data:\", unique_test_labels)\n",
    "        print(\"Known labels from model:\", known_labels)\n",
    "\n",
    "        # (1) Unknown labels in test (not in known_labels)\n",
    "        unknown_labels = list(unique_test_labels - known_labels)\n",
    "        if unknown_labels:\n",
    "            print(f\"\\n Warning: Found unknown labels in test data: {unknown_labels}\")\n",
    "            print(\"These samples will be excluded from evaluation.\")\n",
    "            df_clean = df_clean[df_clean[label_column].isin(known_labels)].reset_index(drop=True)\n",
    "            print(f\"Remaining test samples after filtering unknowns: {len(df_clean)}\")\n",
    "\n",
    "        # (2) Known labels not in test (remove from label2id)\n",
    "        unused_known_labels = known_labels - unique_test_labels\n",
    "        if unused_known_labels:\n",
    "            print(f\"\\nℹ️ Removing unused labels not in test data: {unused_known_labels}\")\n",
    "\n",
    "        # Filter label2id\n",
    "        filtered_label2id = {\n",
    "            label: idx for label, idx in existing_label2id.items()\n",
    "            if label in unique_test_labels\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        # No existing mapping – build one from test data\n",
    "        unique_labels = sorted(df_clean[label_column].unique())\n",
    "        filtered_label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Extract texts and label indices\n",
    "    test_texts = df_clean[text_column].tolist()\n",
    "    test_labels = [filtered_label2id[label] for label in df_clean[label_column].tolist()]\n",
    "\n",
    "    # Extract metadata\n",
    "    available_metadata_cols = [col for col in metadata_columns if col in df_clean.columns]\n",
    "    metadata_df = df_clean[available_metadata_cols].copy()\n",
    "\n",
    "    # Print label distribution\n",
    "    print(\"\\n✅ Test data prepared:\")\n",
    "    print(f\"  Samples: {len(test_texts)}\")\n",
    "    print(f\"  Label distribution:\")\n",
    "    id2label = {v: k for k, v in filtered_label2id.items()}\n",
    "    test_label_names = [id2label[label] for label in test_labels]\n",
    "    print(pd.Series(test_label_names).value_counts())\n",
    "    \n",
    "    return test_texts, test_labels, metadata_df, unknown_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eae3bf4d-bc7e-4ca5-8ba9-07299cbd5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for handling tokenized data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45d0e6f3-8bbd-438e-9330-4827fd91e942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_on_test_data(model_path,tokenizer_path, test_tsv_path, existing_label2id, OUTPUT_DIR_PATH,\n",
    "#                         text_column='Sentence', label_column='Sample_type',\n",
    "#                         batch_size=16, max_length=512):\n",
    "#     \"\"\"\n",
    "#     Perform predictions on test data using a pre-trained model.\n",
    "    \n",
    "#     Args:\n",
    "#         model_path (str): Path to the saved model\n",
    "#         test_tsv_path (str): Path to the test TSV file\n",
    "#         existing_label2id (dict): Label to ID mapping from training\n",
    "#         text_column (str): Name of text column\n",
    "#         label_column (str): Name of label column\n",
    "#         batch_size (int): Batch size for inference\n",
    "#         max_length (int): Maximum sequence length\n",
    "    \n",
    "#     Returns:\n",
    "#         dict: Dictionary containing predictions, true labels, and metrics\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Step 1: Prepare test data\n",
    "#     print(\"=== Preparing Test Data ===\")\n",
    "#     test_texts, test_labels, metadata_df, unknown_labels = prepare_test_data_only(\n",
    "#         test_tsv_path, \n",
    "#         text_column=text_column,\n",
    "#         label_column=label_column,\n",
    "#         existing_label2id=existing_label2id\n",
    "#     )\n",
    "    \n",
    "#     # Step 2: Load model and tokenizer\n",
    "#     print(\"\\n=== Loading Model ===\")\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Step 3: Create dataset and dataloader\n",
    "#     print(\"\\n=== Creating DataLoader ===\")\n",
    "#     test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     # Step 4: Make predictions\n",
    "#     print(\"\\n=== Making Predictions ===\")\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     prediction_probs = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_dataloader:\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "            \n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             logits = outputs.logits\n",
    "            \n",
    "#             # Get predictions\n",
    "#             preds = torch.argmax(logits, dim=-1)\n",
    "#             probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "#             predictions.extend(preds.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#             prediction_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "#     # Step 5: Convert predictions back to label names\n",
    "#     id2label = {v: k for k, v in existing_label2id.items()}\n",
    "#     pred_labels = [id2label[pred] for pred in predictions]\n",
    "#     true_label_names = [id2label[true] for true in true_labels]\n",
    "    \n",
    "#     # Step 6: Calculate metrics\n",
    "#     print(\"\\n=== Evaluation Results ===\")\n",
    "#     accuracy = accuracy_score(true_labels, predictions)\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(true_label_names, pred_labels))\n",
    "    \n",
    "#     print(\"\\nConfusion Matrix:\")\n",
    "#     print(confusion_matrix(true_label_names, pred_labels))\n",
    "\n",
    "#     report_dict = classification_report(true_label_names, pred_labels, output_dict=True)\n",
    "#     accuracy = accuracy_score(true_label_names, pred_labels)\n",
    "#     summary_results = [\n",
    "#     {'metric': 'accuracy', 'class': 'overall', 'value': accuracy},\n",
    "#     {'metric': 'precision', 'class': 'overall', 'value': report_dict['macro avg']['precision']},\n",
    "#     {'metric': 'recall', 'class': 'overall', 'value': report_dict['macro avg']['recall']},\n",
    "#     {'metric': 'f1-score', 'class': 'overall', 'value': report_dict['macro avg']['f1-score']}\n",
    "# ]\n",
    "\n",
    "#     # summary_results = []\n",
    "#     # summary_results.append({\n",
    "#     #     'metric': 'accuracy',\n",
    "#     #     'class': 'overall',\n",
    "#     #     'value': accuracy\n",
    "#     # })\n",
    "    \n",
    "#     for class_name, metrics in report_dict.items():\n",
    "#         if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "#             summary_results.extend([\n",
    "#                 {'metric': 'precision', 'class': class_name, 'value': metrics['precision']},\n",
    "#                 {'metric': 'recall', 'class': class_name, 'value': metrics['recall']},\n",
    "#                 {'metric': 'f1-score', 'class': class_name, 'value': metrics['f1-score']},\n",
    "#                 {'metric': 'support', 'class': class_name, 'value': metrics['support']}\n",
    "#             ])\n",
    "    \n",
    "#     summary_df = pd.DataFrame(summary_results)\n",
    "#     summary_file = os.path.join(OUTPUT_DIR_PATH, f\"_evaluation_summary.csv\")\n",
    "#     summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    \n",
    "#     # Step 7: Create results dataframe\n",
    "#     results_df = metadata_df.copy()\n",
    "#     results_df['text'] = test_texts\n",
    "#     results_df['true_label'] = true_label_names\n",
    "#     results_df['predicted_label'] = pred_labels\n",
    "#     results_df['prediction_confidence'] = [max(probs) for probs in prediction_probs]\n",
    "#     results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "    \n",
    "#     return {\n",
    "#         'predictions': predictions,\n",
    "#         'true_labels': true_labels,\n",
    "#         'pred_labels': pred_labels,\n",
    "#         'true_label_names': true_label_names,\n",
    "#         'prediction_probs': prediction_probs,\n",
    "#         'results_df': results_df,\n",
    "#         'accuracy': accuracy,\n",
    "#         'unknown_labels': unknown_labels\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9db913b-d5ad-44b9-9a62-b4e4caff722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_data(model_path,tokenizer_path, test_tsv_path, existing_label2id, OUTPUT_DIR_PATH,\n",
    "                        text_column='Sentence', label_column='Sample_type',\n",
    "                        batch_size=16, max_length=512):\n",
    "    \"\"\"\n",
    "    Perform predictions on test data using a pre-trained model.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model\n",
    "        test_tsv_path (str): Path to the test TSV file\n",
    "        existing_label2id (dict): Label to ID mapping from training\n",
    "        text_column (str): Name of text column\n",
    "        label_column (str): Name of label column\n",
    "        batch_size (int): Batch size for inference\n",
    "        max_length (int): Maximum sequence length\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing predictions, true labels, and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Prepare test data\n",
    "    print(\"=== Preparing Test Data ===\")\n",
    "    test_texts, test_labels, metadata_df, unknown_labels = prepare_test_data_only(\n",
    "        test_tsv_path, \n",
    "        text_column=text_column,\n",
    "        label_column=label_column,\n",
    "        existing_label2id=existing_label2id\n",
    "    )\n",
    "    \n",
    "    # Step 2: Load model and tokenizer\n",
    "    print(\"\\n=== Loading Model ===\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Step 3: Create dataset and dataloader\n",
    "    print(\"\\n=== Creating DataLoader ===\")\n",
    "    test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Step 4: Make predictions\n",
    "    print(\"\\n=== Making Predictions ===\")\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    prediction_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            prediction_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Step 5: Convert predictions back to label names\n",
    "    id2label = {v: k for k, v in existing_label2id.items()}\n",
    "    pred_labels = [id2label[pred] for pred in predictions]\n",
    "    true_label_names = [id2label[true] for true in true_labels]\n",
    "    \n",
    "    # Step 6: Calculate metrics\n",
    "    \n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_label_names, pred_labels))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(true_label_names, pred_labels))\n",
    "\n",
    "    # Calculate AUC and AUPR\n",
    "    prediction_probs_array = np.array(prediction_probs)\n",
    "    \n",
    "    try:\n",
    "        num_classes_in_test = len(np.unique(true_labels))\n",
    "        num_prob_columns = prediction_probs_array.shape[1]\n",
    "        \n",
    "        print(f\"Debug: Classes in test set: {num_classes_in_test}, Probability columns: {num_prob_columns}\")\n",
    "        \n",
    "        # For binary classification\n",
    "        if num_classes_in_test == 2 and num_prob_columns >= 2:\n",
    "            auc_score = roc_auc_score(true_labels, prediction_probs_array[:, 1])  # Use probability of positive class\n",
    "            aupr_score = average_precision_score(true_labels, prediction_probs_array[:, 1])\n",
    "        elif num_classes_in_test > 2:\n",
    "            # For multi-class - use labels parameter to specify which classes to include\n",
    "            unique_labels = np.unique(true_labels)\n",
    "            if len(unique_labels) == num_prob_columns:\n",
    "                # All classes present in test set\n",
    "                auc_score = roc_auc_score(true_labels, prediction_probs_array, multi_class='ovr', average='macro')\n",
    "                aupr_score = average_precision_score(true_labels, prediction_probs_array, average='macro')\n",
    "            else:\n",
    "                # Some classes missing from test set - use label binarization\n",
    "                from sklearn.preprocessing import label_binarize\n",
    "                from sklearn.metrics import roc_auc_score\n",
    "                \n",
    "                # Get all possible labels from the model (0 to num_prob_columns-1)\n",
    "                all_labels = list(range(num_prob_columns))\n",
    "                \n",
    "                # Binarize true labels for all possible classes\n",
    "                true_labels_binarized = label_binarize(true_labels, classes=all_labels)\n",
    "                \n",
    "                # Calculate AUC for each class and average\n",
    "                auc_scores = []\n",
    "                for i in range(num_prob_columns):\n",
    "                    if i in unique_labels:  # Only calculate for classes present in test set\n",
    "                        class_auc = roc_auc_score(true_labels_binarized[:, i], prediction_probs_array[:, i])\n",
    "                        auc_scores.append(class_auc)\n",
    "                \n",
    "                auc_score = np.mean(auc_scores) if auc_scores else None\n",
    "                \n",
    "                # For AUPR, we'll calculate it differently\n",
    "                aupr_scores = []\n",
    "                for i in range(num_prob_columns):\n",
    "                    if i in unique_labels:\n",
    "                        class_aupr = average_precision_score(true_labels_binarized[:, i], prediction_probs_array[:, i])\n",
    "                        aupr_scores.append(class_aupr)\n",
    "                \n",
    "                aupr_score = np.mean(aupr_scores) if aupr_scores else None\n",
    "        else:\n",
    "            print(\"Warning: Cannot calculate AUC/AUPR - insufficient classes or probability columns\")\n",
    "            auc_score = None\n",
    "            aupr_score = None\n",
    "        \n",
    "        if auc_score is not None:\n",
    "            print(f\"\\nAUC (ROC): {auc_score:.4f}\")\n",
    "        if aupr_score is not None:\n",
    "            print(f\"AUPR (Average Precision): {aupr_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Could not calculate AUC/AUPR - {e}\")\n",
    "        auc_score = None\n",
    "        aupr_score = None\n",
    "\n",
    "    report_dict = classification_report(true_label_names, pred_labels, output_dict=True)\n",
    "    accuracy = accuracy_score(true_label_names, pred_labels)\n",
    "    summary_results = [\n",
    "        {'metric': 'accuracy', 'class': 'overall', 'value': accuracy},\n",
    "        {'metric': 'precision', 'class': 'overall', 'value': report_dict['macro avg']['precision']},\n",
    "        {'metric': 'recall', 'class': 'overall', 'value': report_dict['macro avg']['recall']},\n",
    "        {'metric': 'f1-score', 'class': 'overall', 'value': report_dict['macro avg']['f1-score']}\n",
    "    ]\n",
    "    \n",
    "    # Add AUC and AUPR to summary results if available\n",
    "    if auc_score is not None:\n",
    "        summary_results.append({'metric': 'auc', 'class': 'overall', 'value': auc_score})\n",
    "    if aupr_score is not None:\n",
    "        summary_results.append({'metric': 'aupr', 'class': 'overall', 'value': aupr_score})\n",
    "\n",
    "    for class_name, metrics in report_dict.items():\n",
    "        if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            summary_results.extend([\n",
    "                {'metric': 'precision', 'class': class_name, 'value': metrics['precision']},\n",
    "                {'metric': 'recall', 'class': class_name, 'value': metrics['recall']},\n",
    "                {'metric': 'f1-score', 'class': class_name, 'value': metrics['f1-score']},\n",
    "                {'metric': 'support', 'class': class_name, 'value': metrics['support']}\n",
    "            ])\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    summary_file = os.path.join(OUTPUT_DIR_PATH, f\"_evaluation_summary.csv\")\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    \n",
    "    # Step 7: Create results dataframe\n",
    "    results_df = metadata_df.copy()\n",
    "    results_df['text'] = test_texts\n",
    "    results_df['true_label'] = true_label_names\n",
    "    results_df['predicted_label'] = pred_labels\n",
    "    results_df['prediction_confidence'] = [max(probs) for probs in prediction_probs]\n",
    "    results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'pred_labels': pred_labels,\n",
    "        'true_label_names': true_label_names,\n",
    "        'prediction_probs': prediction_probs,\n",
    "        'results_df': results_df,\n",
    "        'accuracy': accuracy,\n",
    "        'unknown_labels': unknown_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2ed2f931-4d81-4a6d-ac5b-3dc2cfa8ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MOBILE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1e64fad-fdff-4a31-908a-bd88d72c840b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Test Data ===\n",
      "Preparing test data from: /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels/mobile_sentences_and_labels.tsv\n",
      "Loaded 1509 test samples\n",
      "Test samples after cleaning: 1509\n",
      "Labels from test data: {'infant_oral', 'vaginal', 'adult_gut', 'skin', 'adult_oral', 'infant_gut'}\n",
      "Known labels from model: {'adult_gut', 'child_gut', 'vaginal', 'infant_gut'}\n",
      "\n",
      " Warning: Found unknown labels in test data: ['adult_oral', 'infant_oral', 'skin']\n",
      "These samples will be excluded from evaluation.\n",
      "Remaining test samples after filtering unknowns: 1178\n",
      "\n",
      "ℹ️ Removing unused labels not in test data: {'child_gut'}\n",
      "\n",
      "✅ Test data prepared:\n",
      "  Samples: 1178\n",
      "  Label distribution:\n",
      "infant_gut    454\n",
      "vaginal       368\n",
      "adult_gut     356\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Loading Model ===\n",
      "Using device: cuda\n",
      "\n",
      "=== Creating DataLoader ===\n",
      "\n",
      "=== Making Predictions ===\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Accuracy: 0.6995\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   adult_gut       0.98      0.15      0.26       356\n",
      "  infant_gut       0.82      0.97      0.89       454\n",
      "     vaginal       0.56      0.90      0.69       368\n",
      "\n",
      "    accuracy                           0.70      1178\n",
      "   macro avg       0.79      0.67      0.61      1178\n",
      "weighted avg       0.79      0.70      0.64      1178\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 53  60 243]\n",
      " [  0 440  14]\n",
      " [  1  36 331]]\n",
      "Debug: Classes in test set: 3, Probability columns: 4\n",
      "\n",
      "AUC (ROC): 0.9000\n",
      "AUPR (Average Precision): 0.8312\n",
      "\n",
      "Results saved to test_predictions.csv\n",
      "\n",
      "=== Sample Predictions ===\n",
      "Text: Eubacteriales Oscillospiraceae Neobittarella Neobittarella_massiliensis, Eubacteriales Oscillospirac...\n",
      "True: adult_gut, Predicted: adult_gut, Confidence: 0.5186, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Bifidobacteriales Bifidobacteriaceae Gardnerella Gardnerella_vaginalis, Lactobacillales Lactobacilla...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9831, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_crispatus, Lactobacillales Enterococcac...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9858, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Clostridiaceae Clostridium Clostridium_perfringens, Enterobacterales Enterobacteriacea...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9807, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Oscillospiraceae Ruminococcus Ruminococcus_bromii, Eubacteriales Eubacteriales_unclass...\n",
      "True: adult_gut, Predicted: adult_gut, Confidence: 0.8985, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9886, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Bacteroidales Bacteroidaceae Phocaeicola Phocaeicola_dorei, Bacteroidales Bacteroidaceae Phocaeicola...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9830, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Tissierellales Peptoniphilaceae Finegoldia Finegoldia_magna, Tissierellales Peptoniphilaceae Anaeroc...\n",
      "True: adult_gut, Predicted: adult_gut, Confidence: 0.3757, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9899, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Aerococcaceae Falseniella Falseniella_ignava, Bacteroidales Prevotellaceae Prevotell...\n",
      "True: adult_gut, Predicted: adult_gut, Confidence: 0.5150, Correct: True\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_path = f\"{OUTPUT_DIR_sample_type}/model\"\n",
    "tokenizer_path = f\"{OUTPUT_DIR_sample_type}/tokenizer\"\n",
    "test_file_path = TSV_FILE_MOBILE\n",
    "\n",
    "with open(f\"{OUTPUT_DIR_sample_type}/label_mappings.json\", \"r\") as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "label2id = mappings[\"label2id\"]\n",
    "id2label = mappings[\"id2label\"]\n",
    "target_label = mappings[\"target_label\"]\n",
    "\n",
    "\n",
    "\n",
    "# Run predictions\n",
    "results = predict_on_test_data(\n",
    "    model_path=model_path,\n",
    "    tokenizer_path = tokenizer_path,\n",
    "    test_tsv_path=test_file_path,\n",
    "    existing_label2id=label2id,\n",
    "    OUTPUT_DIR_PATH=OUTPUT_DIR_MOBILE,\n",
    "    text_column='Sentence',\n",
    "    label_column='Sample_type',\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results['results_df'].to_csv(f\"{OUTPUT_DIR_MOBILE}/test_predictions.csv\", index=False)\n",
    "print(f\"\\nResults saved to test_predictions.csv\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\n=== Sample Predictions ===\")\n",
    "sample_results = results['results_df'].head(10)\n",
    "for idx, row in sample_results.iterrows():\n",
    "    print(f\"Text: {row['text'][:100]}...\")\n",
    "    print(f\"True: {row['true_label']}, Predicted: {row['predicted_label']}, \"\n",
    "          f\"Confidence: {row['prediction_confidence']:.4f}, \"\n",
    "          f\"Correct: {row['correct']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "42eff27e-1174-41d1-851f-b3908a4dc208",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### comparison to other models: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b302ffb6-7a9b-40cb-a603-fb9503d951e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_on_test_data_comparisons(model,tokenizer, test_tsv_path, existing_label2id, OUTPUT_DIR_PATH, name,\n",
    "#                         text_column='Sentence', label_column='Sample_type',\n",
    "#                         batch_size=16, max_length=512):    \n",
    "#     # Step 1: Prepare test data\n",
    "#     print(\"=== Preparing Test Data ===\")\n",
    "#     test_texts, test_labels, metadata_df, unknown_labels = prepare_test_data_only(\n",
    "#         test_tsv_path, \n",
    "#         text_column=text_column,\n",
    "#         label_column=label_column,\n",
    "#         existing_label2id=existing_label2id\n",
    "#     )\n",
    "    \n",
    "#     # Step 2: Load model and tokenizer\n",
    "#     print(\"\\n=== Loading Model ===\")\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     print(f\"Using device: {device}\")\n",
    "    \n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Step 3: Create dataset and dataloader\n",
    "#     print(\"\\n=== Creating DataLoader ===\")\n",
    "#     test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "#     test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "#     # Step 4: Make predictions\n",
    "#     print(\"\\n=== Making Predictions ===\")\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "#     prediction_probs = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch in test_dataloader:\n",
    "#             input_ids = batch['input_ids'].to(device)\n",
    "#             attention_mask = batch['attention_mask'].to(device)\n",
    "#             labels = batch['labels'].to(device)\n",
    "            \n",
    "#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#             logits = outputs.logits\n",
    "            \n",
    "#             # Get predictions\n",
    "#             preds = torch.argmax(logits, dim=-1)\n",
    "#             probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "#             predictions.extend(preds.cpu().numpy())\n",
    "#             true_labels.extend(labels.cpu().numpy())\n",
    "#             prediction_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "#     # Step 5: Convert predictions back to label names\n",
    "#     id2label = {v: k for k, v in existing_label2id.items()}\n",
    "#     pred_labels = [id2label[pred] for pred in predictions]\n",
    "#     true_label_names = [id2label[true] for true in true_labels]\n",
    "    \n",
    "#     # Step 6: Calculate metrics\n",
    "#     print(\"\\n=== Evaluation Results ===\")\n",
    "#     accuracy = accuracy_score(true_labels, predictions)\n",
    "#     print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(true_label_names, pred_labels))\n",
    "    \n",
    "#     print(\"\\nConfusion Matrix:\")\n",
    "#     print(confusion_matrix(true_label_names, pred_labels))\n",
    "\n",
    "#     report_dict = classification_report(true_label_names, pred_labels, output_dict=True)\n",
    "\n",
    "#     accuracy = accuracy_score(true_label_names, pred_labels)\n",
    "#     summary_results = [\n",
    "#     {'metric': 'accuracy', 'class': 'overall', 'value': accuracy},\n",
    "#     {'metric': 'precision', 'class': 'overall', 'value': report_dict['macro avg']['precision']},\n",
    "#     {'metric': 'recall', 'class': 'overall', 'value': report_dict['macro avg']['recall']},\n",
    "#     {'metric': 'f1-score', 'class': 'overall', 'value': report_dict['macro avg']['f1-score']}\n",
    "# ]\n",
    "    \n",
    "#     for class_name, metrics in report_dict.items():\n",
    "#         if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "#             summary_results.extend([\n",
    "#                 {'metric': 'precision', 'class': class_name, 'value': metrics['precision']},\n",
    "#                 {'metric': 'recall', 'class': class_name, 'value': metrics['recall']},\n",
    "#                 {'metric': 'f1-score', 'class': class_name, 'value': metrics['f1-score']},\n",
    "#                 {'metric': 'support', 'class': class_name, 'value': metrics['support']}\n",
    "#             ])\n",
    "    \n",
    "#     summary_df = pd.DataFrame(summary_results)\n",
    "#     summary_file = os.path.join(OUTPUT_DIR_PATH, f\"{name}_evaluation_summary.csv\")\n",
    "#     summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    \n",
    "#     # Step 7: Create results dataframe\n",
    "#     results_df = metadata_df.copy()\n",
    "#     results_df['text'] = test_texts\n",
    "#     results_df['true_label'] = true_label_names\n",
    "#     results_df['predicted_label'] = pred_labels\n",
    "#     results_df['prediction_confidence'] = [max(probs) for probs in prediction_probs]\n",
    "#     results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "    \n",
    "#     return {\n",
    "#         'predictions': predictions,\n",
    "#         'true_labels': true_labels,\n",
    "#         'pred_labels': pred_labels,\n",
    "#         'true_label_names': true_label_names,\n",
    "#         'prediction_probs': prediction_probs,\n",
    "#         'results_df': results_df,\n",
    "#         'accuracy': accuracy,\n",
    "#         'unknown_labels': unknown_labels\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f5b774f-3b81-436d-b098-3dd5e0e14309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_data_comparisons(model,tokenizer, test_tsv_path, existing_label2id, OUTPUT_DIR_PATH, name,\n",
    "                        text_column='Sentence', label_column='Sample_type',\n",
    "                        batch_size=16, max_length=512):    \n",
    "    # Step 1: Prepare test data\n",
    "    print(\"=== Preparing Test Data ===\")\n",
    "    test_texts, test_labels, metadata_df, unknown_labels = prepare_test_data_only(\n",
    "        test_tsv_path, \n",
    "        text_column=text_column,\n",
    "        label_column=label_column,\n",
    "        existing_label2id=existing_label2id\n",
    "    )\n",
    "    \n",
    "    # Step 2: Load model and tokenizer\n",
    "    print(\"\\n=== Loading Model ===\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Step 3: Create dataset and dataloader\n",
    "    print(\"\\n=== Creating DataLoader ===\")\n",
    "    test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Step 4: Make predictions\n",
    "    print(\"\\n=== Making Predictions ===\")\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    prediction_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            prediction_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # Step 5: Convert predictions back to label names\n",
    "    id2label = {v: k for k, v in existing_label2id.items()}\n",
    "    pred_labels = [id2label[pred] for pred in predictions]\n",
    "    true_label_names = [id2label[true] for true in true_labels]\n",
    "    \n",
    "    # Step 6: Calculate metrics\n",
    "    \n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_label_names, pred_labels))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(true_label_names, pred_labels))\n",
    "\n",
    "    # Calculate AUC and AUPR\n",
    "    prediction_probs_array = np.array(prediction_probs)\n",
    "    \n",
    "    try:\n",
    "        num_classes_in_test = len(np.unique(true_labels))\n",
    "        num_prob_columns = prediction_probs_array.shape[1]\n",
    "        \n",
    "        print(f\"Debug: Classes in test set: {num_classes_in_test}, Probability columns: {num_prob_columns}\")\n",
    "        \n",
    "        # For binary classification\n",
    "        if num_classes_in_test == 2 and num_prob_columns >= 2:\n",
    "            auc_score = roc_auc_score(true_labels, prediction_probs_array[:, 1])  # Use probability of positive class\n",
    "            aupr_score = average_precision_score(true_labels, prediction_probs_array[:, 1])\n",
    "        elif num_classes_in_test > 2:\n",
    "            # For multi-class - use labels parameter to specify which classes to include\n",
    "            unique_labels = np.unique(true_labels)\n",
    "            if len(unique_labels) == num_prob_columns:\n",
    "                # All classes present in test set\n",
    "                auc_score = roc_auc_score(true_labels, prediction_probs_array, multi_class='ovr', average='macro')\n",
    "                aupr_score = average_precision_score(true_labels, prediction_probs_array, average='macro')\n",
    "            else:\n",
    "                # Some classes missing from test set - use label binarization\n",
    "                from sklearn.preprocessing import label_binarize\n",
    "                from sklearn.metrics import roc_auc_score\n",
    "                \n",
    "                # Get all possible labels from the model (0 to num_prob_columns-1)\n",
    "                all_labels = list(range(num_prob_columns))\n",
    "                \n",
    "                # Binarize true labels for all possible classes\n",
    "                true_labels_binarized = label_binarize(true_labels, classes=all_labels)\n",
    "                \n",
    "                # Calculate AUC for each class and average\n",
    "                auc_scores = []\n",
    "                for i in range(num_prob_columns):\n",
    "                    if i in unique_labels:  # Only calculate for classes present in test set\n",
    "                        class_auc = roc_auc_score(true_labels_binarized[:, i], prediction_probs_array[:, i])\n",
    "                        auc_scores.append(class_auc)\n",
    "                \n",
    "                auc_score = np.mean(auc_scores) if auc_scores else None\n",
    "                \n",
    "                # For AUPR, we'll calculate it differently\n",
    "                aupr_scores = []\n",
    "                for i in range(num_prob_columns):\n",
    "                    if i in unique_labels:\n",
    "                        class_aupr = average_precision_score(true_labels_binarized[:, i], prediction_probs_array[:, i])\n",
    "                        aupr_scores.append(class_aupr)\n",
    "                \n",
    "                aupr_score = np.mean(aupr_scores) if aupr_scores else None\n",
    "        else:\n",
    "            print(\"Warning: Cannot calculate AUC/AUPR - insufficient classes or probability columns\")\n",
    "            auc_score = None\n",
    "            aupr_score = None\n",
    "        \n",
    "        if auc_score is not None:\n",
    "            print(f\"\\nAUC (ROC): {auc_score:.4f}\")\n",
    "        if aupr_score is not None:\n",
    "            print(f\"AUPR (Average Precision): {aupr_score:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nWarning: Could not calculate AUC/AUPR - {e}\")\n",
    "        auc_score = None\n",
    "        aupr_score = None\n",
    "\n",
    "    report_dict = classification_report(true_label_names, pred_labels, output_dict=True)\n",
    "    accuracy = accuracy_score(true_label_names, pred_labels)\n",
    "    summary_results = [\n",
    "        {'metric': 'accuracy', 'class': 'overall', 'value': accuracy},\n",
    "        {'metric': 'precision', 'class': 'overall', 'value': report_dict['macro avg']['precision']},\n",
    "        {'metric': 'recall', 'class': 'overall', 'value': report_dict['macro avg']['recall']},\n",
    "        {'metric': 'f1-score', 'class': 'overall', 'value': report_dict['macro avg']['f1-score']}\n",
    "    ]\n",
    "    \n",
    "    # Add AUC and AUPR to summary results if available\n",
    "    if auc_score is not None:\n",
    "        summary_results.append({'metric': 'auc', 'class': 'overall', 'value': auc_score})\n",
    "    if aupr_score is not None:\n",
    "        summary_results.append({'metric': 'aupr', 'class': 'overall', 'value': aupr_score})\n",
    "\n",
    "    for class_name, metrics in report_dict.items():\n",
    "        if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            summary_results.extend([\n",
    "                {'metric': 'precision', 'class': class_name, 'value': metrics['precision']},\n",
    "                {'metric': 'recall', 'class': class_name, 'value': metrics['recall']},\n",
    "                {'metric': 'f1-score', 'class': class_name, 'value': metrics['f1-score']},\n",
    "                {'metric': 'support', 'class': class_name, 'value': metrics['support']}\n",
    "            ])\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    summary_file = os.path.join(OUTPUT_DIR_PATH, f\"{name}_evaluation_summary.csv\")\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    \n",
    "    \n",
    "    # Step 7: Create results dataframe\n",
    "    results_df = metadata_df.copy()\n",
    "    results_df['text'] = test_texts\n",
    "    results_df['true_label'] = true_label_names\n",
    "    results_df['predicted_label'] = pred_labels\n",
    "    results_df['prediction_confidence'] = [max(probs) for probs in prediction_probs]\n",
    "    results_df['correct'] = results_df['true_label'] == results_df['predicted_label']\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels,\n",
    "        'pred_labels': pred_labels,\n",
    "        'true_label_names': true_label_names,\n",
    "        'prediction_probs': prediction_probs,\n",
    "        'results_df': results_df,\n",
    "        'accuracy': accuracy,\n",
    "        'unknown_labels': unknown_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cb4f68b-d15e-41f8-88de-2fdf7d7f9e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def run_model_predictions(MODEL, TOKENIZER, OUTPUT_DIR_PATH, name,\n",
    "                         test_file_path=TSV_FILE_MOBILE, text_column='Sentence', \n",
    "                         label_column='Sample_type', batch_size=16):\n",
    "    \n",
    "    # Load label mappings from the output directory\n",
    "    mappings_path = os.path.join(OUTPUT_DIR_sample_type, \"label_mappings.json\")\n",
    "    \n",
    "    if not os.path.exists(mappings_path):\n",
    "        raise FileNotFoundError(f\"Label mappings not found at {mappings_path}\")\n",
    "    \n",
    "    with open(mappings_path, \"r\") as f:\n",
    "        mappings = json.load(f)\n",
    "    \n",
    "    label2id = mappings[\"label2id\"]\n",
    "    id2label = mappings[\"id2label\"]\n",
    "    target_label = mappings[\"target_label\"]\n",
    "    \n",
    "    if test_file_path is None:\n",
    "        test_file_path = TSV_FILE_MOBILE\n",
    "    \n",
    "    # Run predictions\n",
    "    results = predict_on_test_data_comparisons(\n",
    "        model=MODEL,\n",
    "        tokenizer=TOKENIZER,\n",
    "        test_tsv_path=TSV_FILE_MOBILE,\n",
    "        existing_label2id=label2id,\n",
    "        OUTPUT_DIR_PATH=OUTPUT_DIR_PATH,\n",
    "        name=name,\n",
    "        text_column='Sentence',\n",
    "        label_column='Sample_type',\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    output_file = os.path.join(OUTPUT_DIR_PATH, f\"{name}.csv\")\n",
    "    results['results_df'].to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(\"\\n=== Sample Predictions ===\")\n",
    "    sample_results = results['results_df'].head(10)\n",
    "    for idx, row in sample_results.iterrows():\n",
    "        print(f\"Text: {row['text'][:100]}...\")\n",
    "        print(f\"True: {row['true_label']}, Predicted: {row['predicted_label']}, \"\n",
    "              f\"Confidence: {row['prediction_confidence']:.4f}, \"\n",
    "              f\"Correct: {row['correct']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fc1c06dd-3279-4e31-b22f-1ea65613f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR_MOBILE_comparisons = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ede979e7-71cb-43fe-be23-9652cb53f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results of a regular BERT model - with our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d2f26f7d-2fb9-4e2e-a3dc-0f1f7a2811bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR_sample_type_BERT = '/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/Sample_type/Bert/'\n",
    "model_path = f\"{OUTPUT_DIR_sample_type_BERT}/model\"\n",
    "tokenizer_path = f\"{OUTPUT_DIR_sample_type_BERT}/tokenizer\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d442f46d-fb08-4052-b04c-65d3b2c02907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Test Data ===\n",
      "Preparing test data from: /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels/mobile_sentences_and_labels.tsv\n",
      "Loaded 1509 test samples\n",
      "Test samples after cleaning: 1509\n",
      "Labels from test data: {'infant_oral', 'vaginal', 'adult_gut', 'skin', 'adult_oral', 'infant_gut'}\n",
      "Known labels from model: {'adult_gut', 'child_gut', 'vaginal', 'infant_gut'}\n",
      "\n",
      " Warning: Found unknown labels in test data: ['adult_oral', 'infant_oral', 'skin']\n",
      "These samples will be excluded from evaluation.\n",
      "Remaining test samples after filtering unknowns: 1178\n",
      "\n",
      "ℹ️ Removing unused labels not in test data: {'child_gut'}\n",
      "\n",
      "✅ Test data prepared:\n",
      "  Samples: 1178\n",
      "  Label distribution:\n",
      "infant_gut    454\n",
      "vaginal       368\n",
      "adult_gut     356\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Loading Model ===\n",
      "Using device: cuda\n",
      "\n",
      "=== Creating DataLoader ===\n",
      "\n",
      "=== Making Predictions ===\n",
      "['infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'adult_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'adult_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'vaginal']\n",
      "['adult_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'adult_gut', 'vaginal', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal', 'infant_gut', 'infant_gut', 'adult_gut', 'vaginal']\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Accuracy: 0.6825\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   adult_gut       0.96      0.15      0.27       356\n",
      "  infant_gut       0.69      0.97      0.81       454\n",
      "     vaginal       0.64      0.83      0.72       368\n",
      "\n",
      "    accuracy                           0.68      1178\n",
      "   macro avg       0.76      0.65      0.60      1178\n",
      "weighted avg       0.76      0.68      0.62      1178\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 55 135 166]\n",
      " [  2 442  10]\n",
      " [  0  61 307]]\n",
      "Debug: Classes in test set: 3, Probability columns: 4\n",
      "\n",
      "AUC (ROC): 0.8436\n",
      "AUPR (Average Precision): 0.7463\n",
      "\n",
      "Results saved to /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results/Bert_our_tokenizer.csv\n",
      "\n",
      "=== Sample Predictions ===\n",
      "Text: Eubacteriales Oscillospiraceae Neobittarella Neobittarella_massiliensis, Eubacteriales Oscillospirac...\n",
      "True: adult_gut, Predicted: infant_gut, Confidence: 0.6923, Correct: False\n",
      "--------------------------------------------------\n",
      "Text: Bifidobacteriales Bifidobacteriaceae Gardnerella Gardnerella_vaginalis, Lactobacillales Lactobacilla...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9659, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_crispatus, Lactobacillales Enterococcac...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9501, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Clostridiaceae Clostridium Clostridium_perfringens, Enterobacterales Enterobacteriacea...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9700, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Oscillospiraceae Ruminococcus Ruminococcus_bromii, Eubacteriales Eubacteriales_unclass...\n",
      "True: adult_gut, Predicted: adult_gut, Confidence: 0.8852, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.8964, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Bacteroidales Bacteroidaceae Phocaeicola Phocaeicola_dorei, Bacteroidales Bacteroidaceae Phocaeicola...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9667, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Tissierellales Peptoniphilaceae Finegoldia Finegoldia_magna, Tissierellales Peptoniphilaceae Anaeroc...\n",
      "True: adult_gut, Predicted: vaginal, Confidence: 0.6886, Correct: False\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9670, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Aerococcaceae Falseniella Falseniella_ignava, Bacteroidales Prevotellaceae Prevotell...\n",
      "True: adult_gut, Predicted: infant_gut, Confidence: 0.4308, Correct: False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = run_model_predictions(\n",
    "    MODEL=model,\n",
    "    TOKENIZER=tokenizer, \n",
    "    OUTPUT_DIR_PATH=OUTPUT_DIR_MOBILE_comparisons,\n",
    "    name=\"Bert_our_tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78b91c-ac53-485c-8d57-ad672ea9e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results of a model with only species level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac17ef07-04b1-462a-8885-04bab66b4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR_SPECIES_LEVEL = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model_species_level/Sample_type\"\n",
    "\n",
    "model_path = f\"{OUTPUT_DIR_SPECIES_LEVEL}/model\"\n",
    "tokenizer_path = f\"{OUTPUT_DIR_SPECIES_LEVEL}/tokenizer\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de5ac0ab-ee23-44c9-ad17-829aef8a3309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Test Data ===\n",
      "Preparing test data from: /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels/mobile_sentences_and_labels.tsv\n",
      "Loaded 1509 test samples\n",
      "Test samples after cleaning: 1509\n",
      "Labels from test data: {'infant_oral', 'vaginal', 'adult_gut', 'skin', 'adult_oral', 'infant_gut'}\n",
      "Known labels from model: {'adult_gut', 'child_gut', 'vaginal', 'infant_gut'}\n",
      "\n",
      " Warning: Found unknown labels in test data: ['adult_oral', 'infant_oral', 'skin']\n",
      "These samples will be excluded from evaluation.\n",
      "Remaining test samples after filtering unknowns: 1178\n",
      "\n",
      "ℹ️ Removing unused labels not in test data: {'child_gut'}\n",
      "\n",
      "✅ Test data prepared:\n",
      "  Samples: 1178\n",
      "  Label distribution:\n",
      "infant_gut    454\n",
      "vaginal       368\n",
      "adult_gut     356\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Loading Model ===\n",
      "Using device: cuda\n",
      "\n",
      "=== Creating DataLoader ===\n",
      "\n",
      "=== Making Predictions ===\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Accuracy: 0.5891\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   adult_gut       0.00      0.00      0.00       356\n",
      "  infant_gut       0.62      0.88      0.73       454\n",
      "     vaginal       0.55      0.80      0.65       368\n",
      "\n",
      "    accuracy                           0.59      1178\n",
      "   macro avg       0.39      0.56      0.46      1178\n",
      "weighted avg       0.41      0.59      0.48      1178\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 173 183]\n",
      " [  0 399  55]\n",
      " [  0  73 295]]\n",
      "Debug: Classes in test set: 3, Probability columns: 4\n",
      "\n",
      "AUC (ROC): 0.8772\n",
      "AUPR (Average Precision): 0.8049\n",
      "\n",
      "Results saved to /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results/species_level_model_our_tokenizer.csv\n",
      "\n",
      "=== Sample Predictions ===\n",
      "Text: Eubacteriales Oscillospiraceae Neobittarella Neobittarella_massiliensis, Eubacteriales Oscillospirac...\n",
      "True: adult_gut, Predicted: infant_gut, Confidence: 0.9320, Correct: False\n",
      "--------------------------------------------------\n",
      "Text: Bifidobacteriales Bifidobacteriaceae Gardnerella Gardnerella_vaginalis, Lactobacillales Lactobacilla...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9423, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_crispatus, Lactobacillales Enterococcac...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9470, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Clostridiaceae Clostridium Clostridium_perfringens, Enterobacterales Enterobacteriacea...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9482, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Oscillospiraceae Ruminococcus Ruminococcus_bromii, Eubacteriales Eubacteriales_unclass...\n",
      "True: adult_gut, Predicted: vaginal, Confidence: 0.4994, Correct: False\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.8014, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Bacteroidales Bacteroidaceae Phocaeicola Phocaeicola_dorei, Bacteroidales Bacteroidaceae Phocaeicola...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9542, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Tissierellales Peptoniphilaceae Finegoldia Finegoldia_magna, Tissierellales Peptoniphilaceae Anaeroc...\n",
      "True: adult_gut, Predicted: vaginal, Confidence: 0.6655, Correct: False\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9663, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Aerococcaceae Falseniella Falseniella_ignava, Bacteroidales Prevotellaceae Prevotell...\n",
      "True: adult_gut, Predicted: vaginal, Confidence: 0.5556, Correct: False\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/sci/labs/morani/morani/icore-data/lab/Tools/personal_condas/chen/miniforge3/envs/anlp_project_jupyter_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = run_model_predictions(\n",
    "    MODEL=model,\n",
    "    TOKENIZER=tokenizer, \n",
    "    OUTPUT_DIR_PATH=OUTPUT_DIR_MOBILE_comparisons,\n",
    "    name=\"species_level_model_our_tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bd9bd0d7-e9f5-4293-ac5c-8d2b7ec31bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biomed Normal tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2fcaa5ae-c0f0-42f3-9dbb-f7cbd7b4cf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_BIOMED = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "OUTPUT_DIR_sample_type_BIOMED = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/Sample_type/Biomed\"\n",
    "\n",
    "model_path = f\"{OUTPUT_DIR_sample_type_BIOMED}/model\"\n",
    "tokenizer_path = f\"{OUTPUT_DIR_sample_type_BIOMED}/tokenizer\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "20c92ee7-81cc-4f8e-b199-2f944fc1ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Test Data ===\n",
      "Preparing test data from: /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/sentences_with_labels/mobile_sentences_and_labels.tsv\n",
      "Loaded 1509 test samples\n",
      "Test samples after cleaning: 1509\n",
      "Labels from test data: {'infant_oral', 'vaginal', 'adult_gut', 'skin', 'adult_oral', 'infant_gut'}\n",
      "Known labels from model: {'adult_gut', 'child_gut', 'vaginal', 'infant_gut'}\n",
      "\n",
      " Warning: Found unknown labels in test data: ['adult_oral', 'infant_oral', 'skin']\n",
      "These samples will be excluded from evaluation.\n",
      "Remaining test samples after filtering unknowns: 1178\n",
      "\n",
      "ℹ️ Removing unused labels not in test data: {'child_gut'}\n",
      "\n",
      "✅ Test data prepared:\n",
      "  Samples: 1178\n",
      "  Label distribution:\n",
      "infant_gut    454\n",
      "vaginal       368\n",
      "adult_gut     356\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Loading Model ===\n",
      "Using device: cuda\n",
      "\n",
      "=== Creating DataLoader ===\n",
      "\n",
      "=== Making Predictions ===\n",
      "\n",
      "=== Evaluation Results ===\n",
      "Accuracy: 0.7029\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   adult_gut       0.98      0.17      0.29       356\n",
      "  infant_gut       0.77      0.97      0.86       454\n",
      "     vaginal       0.60      0.89      0.72       368\n",
      "\n",
      "    accuracy                           0.70      1178\n",
      "   macro avg       0.78      0.68      0.62      1178\n",
      "weighted avg       0.78      0.70      0.64      1178\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 60  93 203]\n",
      " [  0 440  14]\n",
      " [  1  39 328]]\n",
      "Debug: Classes in test set: 3, Probability columns: 4\n",
      "\n",
      "AUC (ROC): 0.9005\n",
      "AUPR (Average Precision): 0.8259\n",
      "\n",
      "Results saved to /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results/Biomed_model_regular_tokenizer.csv\n",
      "\n",
      "=== Sample Predictions ===\n",
      "Text: Eubacteriales Oscillospiraceae Neobittarella Neobittarella_massiliensis, Eubacteriales Oscillospirac...\n",
      "True: adult_gut, Predicted: adult_gut, Confidence: 0.5390, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Bifidobacteriales Bifidobacteriaceae Gardnerella Gardnerella_vaginalis, Lactobacillales Lactobacilla...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9847, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_crispatus, Lactobacillales Enterococcac...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.8544, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Clostridiaceae Clostridium Clostridium_perfringens, Enterobacterales Enterobacteriacea...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9775, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Eubacteriales Oscillospiraceae Ruminococcus Ruminococcus_bromii, Eubacteriales Eubacteriales_unclass...\n",
      "True: adult_gut, Predicted: adult_gut, Confidence: 0.9484, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9765, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Bacteroidales Bacteroidaceae Phocaeicola Phocaeicola_dorei, Bacteroidales Bacteroidaceae Phocaeicola...\n",
      "True: infant_gut, Predicted: infant_gut, Confidence: 0.9857, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Tissierellales Peptoniphilaceae Finegoldia Finegoldia_magna, Tissierellales Peptoniphilaceae Anaeroc...\n",
      "True: adult_gut, Predicted: infant_gut, Confidence: 0.4582, Correct: False\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Lactobacillaceae Lactobacillus Lactobacillus_iners, Lactobacillales Lactobacillaceae...\n",
      "True: vaginal, Predicted: vaginal, Confidence: 0.9877, Correct: True\n",
      "--------------------------------------------------\n",
      "Text: Lactobacillales Aerococcaceae Falseniella Falseniella_ignava, Bacteroidales Prevotellaceae Prevotell...\n",
      "True: adult_gut, Predicted: vaginal, Confidence: 0.8034, Correct: False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = run_model_predictions(\n",
    "    MODEL=model,\n",
    "    TOKENIZER=tokenizer, \n",
    "    OUTPUT_DIR_PATH=OUTPUT_DIR_MOBILE_comparisons,\n",
    "    name=\"Biomed_model_regular_tokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "819fc427-4571-4511-95de-26630143035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## results ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fb3b7267-c233-422a-9a29-92b41284c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def create_model_comparison_table(results_directory, output_file=None):\n",
    "    \n",
    "    pattern = os.path.join(results_directory, \"*_evaluation_summary.csv\")\n",
    "    csv_files = glob.glob(pattern)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No evaluation summary files found in {results_directory}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} evaluation files:\")\n",
    "    for file in csv_files:\n",
    "        print(f\"  - {os.path.basename(file)}\")\n",
    "    \n",
    "    comparison_data = {}\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        model_name = os.path.basename(csv_file).replace('_evaluation_summary.csv', '')\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            model_metrics = {}\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                metric_name = row['metric']\n",
    "                class_name = row['class']\n",
    "                value = row['value']\n",
    "\n",
    "                if class_name == 'overall':\n",
    "                    column_name = metric_name\n",
    "                else:\n",
    "                    column_name = f\"{class_name}_{metric_name}\"\n",
    "                \n",
    "                model_metrics[column_name] = value\n",
    "            \n",
    "            comparison_data[model_name] = model_metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {csv_file}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    comparison_df = pd.DataFrame.from_dict(comparison_data, orient='index')\n",
    "\n",
    "    overall_cols = [col for col in comparison_df.columns if not '_' in col or col.startswith(('macro', 'weighted'))]\n",
    "    class_cols = [col for col in comparison_df.columns if col not in overall_cols]\n",
    "\n",
    "    overall_cols.sort()\n",
    "    class_cols.sort()\n",
    "\n",
    "    ordered_cols = overall_cols + class_cols\n",
    "    comparison_df = comparison_df[ordered_cols]\n",
    "\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    print(f\"\\nComparison table created with shape: {comparison_df.shape}\")\n",
    "    print(f\"Models: {list(comparison_df.index)}\")\n",
    "    print(f\"Metrics: {len(comparison_df.columns)}\")\n",
    "\n",
    "    if output_file:\n",
    "        comparison_df.to_csv(output_file)\n",
    "        print(f\"Comparison table saved to: {output_file}\")\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c504b9dc-0223-4be7-884f-2bfa127bc8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 evaluation files:\n",
      "  - Bert_our_tokenizer_evaluation_summary.csv\n",
      "  - Biomed_model_regular_tokenizer_evaluation_summary.csv\n",
      "  - our_model_evaluation_summary.csv\n",
      "  - species_level_model_our_tokenizer_evaluation_summary.csv\n",
      "\n",
      "Comparison table created with shape: (4, 18)\n",
      "Models: ['Bert_our_tokenizer', 'Biomed_model_regular_tokenizer', 'our_model', 'species_level_model_our_tokenizer']\n",
      "Metrics: 18\n",
      "Comparison table saved to: /sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results/full_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "results_dir = \"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results/\"\n",
    "    \n",
    "    # Create full comparison table\n",
    "full_comparison = create_model_comparison_table(\n",
    "    results_directory=results_dir,\n",
    "    output_file=\"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results/full_model_comparison.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4295c354-a08d-466e-9ff7-734b7928d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd.read_csv(\"/sci/backup/morani/lab/Projects/Aluma/ANLP/Project/supervised_model/MOBILE/comparisons_results/full_model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "716ba343-ee56-449b-8957-6f41ab83ee14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "      <th>aupr</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>adult_gut_f1-score</th>\n",
       "      <th>adult_gut_precision</th>\n",
       "      <th>adult_gut_recall</th>\n",
       "      <th>adult_gut_support</th>\n",
       "      <th>infant_gut_f1-score</th>\n",
       "      <th>infant_gut_precision</th>\n",
       "      <th>infant_gut_recall</th>\n",
       "      <th>infant_gut_support</th>\n",
       "      <th>vaginal_f1-score</th>\n",
       "      <th>vaginal_precision</th>\n",
       "      <th>vaginal_recall</th>\n",
       "      <th>vaginal_support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bert_our_tokenizer</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.8436</td>\n",
       "      <td>0.7463</td>\n",
       "      <td>0.5991</td>\n",
       "      <td>0.7644</td>\n",
       "      <td>0.6541</td>\n",
       "      <td>0.2663</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.1545</td>\n",
       "      <td>356.0</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.6928</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>454.0</td>\n",
       "      <td>0.7215</td>\n",
       "      <td>0.6356</td>\n",
       "      <td>0.8342</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Biomed_model_regular_tokenizer</td>\n",
       "      <td>0.7029</td>\n",
       "      <td>0.9005</td>\n",
       "      <td>0.8259</td>\n",
       "      <td>0.6213</td>\n",
       "      <td>0.7849</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>0.2878</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>0.1685</td>\n",
       "      <td>356.0</td>\n",
       "      <td>0.8577</td>\n",
       "      <td>0.7692</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>454.0</td>\n",
       "      <td>0.7185</td>\n",
       "      <td>0.6018</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>our_model</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.8312</td>\n",
       "      <td>0.6133</td>\n",
       "      <td>0.7884</td>\n",
       "      <td>0.6725</td>\n",
       "      <td>0.2585</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.1489</td>\n",
       "      <td>356.0</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.8209</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>454.0</td>\n",
       "      <td>0.6925</td>\n",
       "      <td>0.5629</td>\n",
       "      <td>0.8995</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>species_level_model_our_tokenizer</td>\n",
       "      <td>0.5891</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>0.8049</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.3907</td>\n",
       "      <td>0.5602</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>356.0</td>\n",
       "      <td>0.7261</td>\n",
       "      <td>0.6186</td>\n",
       "      <td>0.8789</td>\n",
       "      <td>454.0</td>\n",
       "      <td>0.6548</td>\n",
       "      <td>0.5535</td>\n",
       "      <td>0.8016</td>\n",
       "      <td>368.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Unnamed: 0  accuracy     auc    aupr  f1-score  \\\n",
       "0                 Bert_our_tokenizer    0.6825  0.8436  0.7463    0.5991   \n",
       "1     Biomed_model_regular_tokenizer    0.7029  0.9005  0.8259    0.6213   \n",
       "2                          our_model    0.6995  0.9000  0.8312    0.6133   \n",
       "3  species_level_model_our_tokenizer    0.5891  0.8772  0.8049    0.4603   \n",
       "\n",
       "   precision  recall  adult_gut_f1-score  adult_gut_precision  \\\n",
       "0     0.7644  0.6541              0.2663               0.9649   \n",
       "1     0.7849  0.6763              0.2878               0.9836   \n",
       "2     0.7884  0.6725              0.2585               0.9815   \n",
       "3     0.3907  0.5602              0.0000               0.0000   \n",
       "\n",
       "   adult_gut_recall  adult_gut_support  infant_gut_f1-score  \\\n",
       "0            0.1545              356.0               0.8095   \n",
       "1            0.1685              356.0               0.8577   \n",
       "2            0.1489              356.0               0.8889   \n",
       "3            0.0000              356.0               0.7261   \n",
       "\n",
       "   infant_gut_precision  infant_gut_recall  infant_gut_support  \\\n",
       "0                0.6928             0.9736               454.0   \n",
       "1                0.7692             0.9692               454.0   \n",
       "2                0.8209             0.9692               454.0   \n",
       "3                0.6186             0.8789               454.0   \n",
       "\n",
       "   vaginal_f1-score  vaginal_precision  vaginal_recall  vaginal_support  \n",
       "0            0.7215             0.6356          0.8342            368.0  \n",
       "1            0.7185             0.6018          0.8913            368.0  \n",
       "2            0.6925             0.5629          0.8995            368.0  \n",
       "3            0.6548             0.5535          0.8016            368.0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2b5e2-4b46-422e-aa1a-cd2c44c8844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GMAP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9282afcb-02ab-435d-8593-256bf69e984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{OUTPUT_DIR_age}/model\"\n",
    "tokenizer_path = f\"{OUTPUT_DIR_age}/tokenizer\"\n",
    "test_file_path = TSV_FILE_GMAP\n",
    "\n",
    "with open(f\"{OUTPUT_DIR_age}/label_mappings.json\", \"r\") as f:\n",
    "    mappings = json.load(f)\n",
    "\n",
    "label2id = mappings[\"label2id\"]\n",
    "id2label = mappings[\"id2label\"]\n",
    "target_label = mappings[\"target_label\"]\n",
    "\n",
    "\n",
    "# Run predictions\n",
    "results = predict_on_test_data(\n",
    "    model_path=model_path,\n",
    "    tokenizer_path = tokenizer_path,\n",
    "    test_tsv_path=test_file_path,\n",
    "    existing_label2id=label2id,\n",
    "    text_column='Sentence',\n",
    "    label_column='Age',\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results['results_df'].to_csv(f\"{OUTPUT_DIR_GMAP}/test_predictions.csv\", index=False)\n",
    "\n",
    "print(f\"\\nResults saved to test_predictions.csv\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\n=== Sample Predictions ===\")\n",
    "sample_results = results['results_df'].head(10)\n",
    "for idx, row in sample_results.iterrows():\n",
    "    print(f\"Text: {row['text'][:100]}...\")\n",
    "    print(f\"True: {row['true_label']}, Predicted: {row['predicted_label']}, \"\n",
    "          f\"Confidence: {row['prediction_confidence']:.4f}, \"\n",
    "          f\"Correct: {row['correct']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anlp_project_jupyter_env",
   "language": "python",
   "name": "anlp_project_jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
